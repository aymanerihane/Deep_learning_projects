{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Ek_azYDeLeM1",
        "outputId": "43ab5d6a-f30c-4d98-c370-76c39f4fea21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mediapipe\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (25.1.24)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.4.33)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (1.26.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe) (4.25.6)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe)\n",
            "  Downloading sounddevice-0.5.1-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe) (1.17.1)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (0.4.1)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.10 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe) (1.13.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (4.55.8)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe) (2.8.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.1-py3-none-any.whl (32 kB)\n",
            "Installing collected packages: sounddevice, mediapipe\n",
            "Successfully installed mediapipe-0.10.21 sounddevice-0.5.1\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pw-zkxryLeM4"
      },
      "outputs": [],
      "source": [
        "# Expressions labels\n",
        "expressions = [\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "train_data_dir = '/content/Dataset/train/'\n",
        "test_data_dir = '/content/Dataset/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2WsY27kbLeM5"
      },
      "outputs": [],
      "source": [
        "# Moving window for smoothing predictions\n",
        "PREDICTION_WINDOW = deque(maxlen=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bVT4TpCBLeM6"
      },
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False, max_num_faces=5, min_detection_confidence=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gxdy5omILeM7"
      },
      "outputs": [],
      "source": [
        "def build_model(type):\n",
        "    \"\"\"Build the CNN model.\"\"\"\n",
        "    from tensorflow.keras.applications import VGG16, VGG19\n",
        "    #  Load VGG19 with pretrained ImageNet weights\n",
        "    if type == 'vgg16':\n",
        "        model = VGG16(weights='imagenet')\n",
        "    else:\n",
        "      model = VGG19(weights='imagenet')\n",
        "\n",
        "    return model\n",
        "build_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMopAbzoLeM9"
      },
      "outputs": [],
      "source": [
        "def train_model(model, train_dir, test_dir, epochs=50):\n",
        "    \"\"\"Train the model with given data.\"\"\"\n",
        "    # Data augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=30,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.3,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        test_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "\n",
        "    num_train_imgs = 0\n",
        "    for root, dirs, files in os.walk(train_data_dir):\n",
        "        num_train_imgs += len(files)\n",
        "    num_test_imgs = 0\n",
        "    for root, dirs, files in os.walk(test_data_dir):\n",
        "        num_test_imgs += len(files)\n",
        "\n",
        "    print(num_train_imgs)\n",
        "    print(num_test_imgs)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=num_train_imgs // 32,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=num_test_imgs // 32,\n",
        "\n",
        "    )\n",
        "\n",
        "    model.save(\"model_CNN.h5\")\n",
        "\n",
        "    return history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gD0iish2LeM_"
      },
      "outputs": [],
      "source": [
        "def preprocess_face(face_img):\n",
        "    \"\"\"Preprocess a face image for prediction.\"\"\"\n",
        "    gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (48, 48))\n",
        "    normalized = resized / 255.0\n",
        "    return normalized.reshape(1, 48, 48, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gruQbXBLeNA"
      },
      "outputs": [],
      "source": [
        "def detect_expression(frame, model):\n",
        "    \"\"\"Detect facial expression in a video frame.\"\"\"\n",
        "    height, width, _ = frame.shape\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb_frame)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            x_min = y_min = float(\"inf\")\n",
        "            x_max = y_max = float(\"-inf\")\n",
        "\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                x, y = int(landmark.x * width), int(landmark.y * height)\n",
        "                x_min = min(x_min, x)\n",
        "                x_max = max(x_max, x)\n",
        "                y_min = min(y_min, y)\n",
        "                y_max = max(y_max, y)\n",
        "\n",
        "            padding = 20\n",
        "            x_min = max(0, x_min - padding)\n",
        "            y_min = max(0, y_min - padding)\n",
        "            x_max = min(width, x_max + padding)\n",
        "            y_max = min(height, y_max + padding)\n",
        "\n",
        "            face = frame[y_min:y_max, x_min:x_max]\n",
        "            if face.size == 0:\n",
        "                continue\n",
        "\n",
        "            processed_face = preprocess_face(face)\n",
        "            prediction = model.predict(processed_face)[0]\n",
        "            PREDICTION_WINDOW.append(prediction)\n",
        "\n",
        "            avg_prediction = np.mean(PREDICTION_WINDOW, axis=0)\n",
        "            expression = expressions[np.argmax(avg_prediction)]\n",
        "            confidence = float(avg_prediction.max())\n",
        "\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"{expression} ({confidence:.2f})\",\n",
        "                (x_min, y_min - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9,\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkudQH-5LeNB"
      },
      "outputs": [],
      "source": [
        "def run_realtime(model):\n",
        "    \"\"\"Run real-time facial expression detection.\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        processed_frame = detect_expression(frame, model)\n",
        "\n",
        "        cv2.imshow(\"Facial Expression Detection\", processed_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3PYYVmzFLeNC"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dir, history):\n",
        "    \"\"\"Evaluate the model and visualize the results.\"\"\"\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        color_mode=\"grayscale\",\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # Get predictions and calculate metrics\n",
        "    predictions = model.predict(test_generator)\n",
        "    true_labels = test_generator.classes\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    report = classification_report(\n",
        "        true_labels, predicted_labels, target_names=expressions\n",
        "    )\n",
        "    accuracy = np.sum(true_labels == predicted_labels) / len(true_labels)\n",
        "\n",
        "    print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=expressions,\n",
        "        yticklabels=expressions,\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Accuracy and Loss over Epochs\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OJLtUJJDLeND"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    types = ['vgg16','vgg19']\n",
        "    for tp in types:\n",
        "      # Build the model\n",
        "      model = build_model(tp)\n",
        "\n",
        "      # Train the model (optional, you can load a pretrained model)\n",
        "      train = input(\"Do you want to train the model? (y/n): \")\n",
        "      if train.lower() == 'y':\n",
        "          history = train_model(model, train_data_dir, test_data_dir, epochs=60)\n",
        "      else:\n",
        "          # Load pre-trained model (if available)\n",
        "          if os.path.exists(f\"model_{tp}.h5\"):\n",
        "              model = load_model(\"model_CNN.h5\")\n",
        "              print(\"Model loaded successfully.\")\n",
        "          else:\n",
        "              print(\"No pre-trained model found. You should train the model first.\")\n",
        "              return\n",
        "\n",
        "      # Evaluate the model on the test set\n",
        "      evaluate = input(\"Do you want to evaluate the model? (y/n): \")\n",
        "      if evaluate.lower() == 'y':\n",
        "          evaluate_model(model, test_data_dir, history)\n",
        "\n",
        "      # Run real-time facial expression detection\n",
        "      run_realtime_choice = input(\"Do you want to run real-time facial expression detection? (y/n): \")\n",
        "      if run_realtime_choice.lower() == 'y':\n",
        "          run_realtime(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GXR7kiWFLeNE"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}