{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Ek_azYDeLeM1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "pw-zkxryLeM4"
      },
      "outputs": [],
      "source": [
        "# Expressions labels\n",
        "expressions = [\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "train_data_dir = '/content/Dataset/train/'\n",
        "test_data_dir = '/content/Dataset/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2WsY27kbLeM5"
      },
      "outputs": [],
      "source": [
        "# Moving window for smoothing predictions\n",
        "PREDICTION_WINDOW = deque(maxlen=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bVT4TpCBLeM6"
      },
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False, max_num_faces=5, min_detection_confidence=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Gxdy5omILeM7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "\n",
        "\n",
        "def build_model(type):\n",
        "    \"\"\"Build the CNN model.\"\"\"\n",
        "    #  Load VGG19 with pretrained ImageNet weights\n",
        "    if type == 'vgg16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "    else:\n",
        "        base_model = VGG19(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "\n",
        "    # Freeze the base layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom classification head\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(7, activation='softmax')(x)  # Example: 10 classes\n",
        "\n",
        "    # Create new model\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "# build_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(tp,model, train_dir, test_dir, epochs=50):\n",
        "    \"\"\"Train the model with given data.\"\"\"\n",
        "    # Data augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=30,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.3,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        test_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48,48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "\n",
        "    num_train_imgs = 0\n",
        "    for root, dirs, files in os.walk(train_data_dir):\n",
        "        num_train_imgs += len(files)\n",
        "    num_test_imgs = 0\n",
        "    for root, dirs, files in os.walk(test_data_dir):\n",
        "        num_test_imgs += len(files)\n",
        "\n",
        "    print(num_train_imgs)\n",
        "    print(num_test_imgs)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=num_train_imgs // 32,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=num_test_imgs // 32,\n",
        "\n",
        "    )\n",
        "\n",
        "    model.save(f\"model_{tp}.h5\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "qhYoXg8ZOE-s"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "gD0iish2LeM_"
      },
      "outputs": [],
      "source": [
        "def preprocess_face(face_img):\n",
        "    \"\"\"Preprocess a face image for prediction.\"\"\"\n",
        "    gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (48, 48))\n",
        "    normalized = resized / 255.0\n",
        "    return normalized.reshape(1, 48, 48, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "0gruQbXBLeNA"
      },
      "outputs": [],
      "source": [
        "def detect_expression(frame, model):\n",
        "    \"\"\"Detect facial expression in a video frame.\"\"\"\n",
        "    height, width, _ = frame.shape\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb_frame)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            x_min = y_min = float(\"inf\")\n",
        "            x_max = y_max = float(\"-inf\")\n",
        "\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                x, y = int(landmark.x * width), int(landmark.y * height)\n",
        "                x_min = min(x_min, x)\n",
        "                x_max = max(x_max, x)\n",
        "                y_min = min(y_min, y)\n",
        "                y_max = max(y_max, y)\n",
        "\n",
        "            padding = 20\n",
        "            x_min = max(0, x_min - padding)\n",
        "            y_min = max(0, y_min - padding)\n",
        "            x_max = min(width, x_max + padding)\n",
        "            y_max = min(height, y_max + padding)\n",
        "\n",
        "            face = frame[y_min:y_max, x_min:x_max]\n",
        "            if face.size == 0:\n",
        "                continue\n",
        "\n",
        "            processed_face = preprocess_face(face)\n",
        "            prediction = model.predict(processed_face)[0]\n",
        "            PREDICTION_WINDOW.append(prediction)\n",
        "\n",
        "            avg_prediction = np.mean(PREDICTION_WINDOW, axis=0)\n",
        "            expression = expressions[np.argmax(avg_prediction)]\n",
        "            confidence = float(avg_prediction.max())\n",
        "\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"{expression} ({confidence:.2f})\",\n",
        "                (x_min, y_min - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9,\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "MkudQH-5LeNB"
      },
      "outputs": [],
      "source": [
        "def run_realtime(model):\n",
        "    \"\"\"Run real-time facial expression detection.\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        processed_frame = detect_expression(frame, model)\n",
        "\n",
        "        cv2.imshow(\"Facial Expression Detection\", processed_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3PYYVmzFLeNC"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dir, history):\n",
        "    \"\"\"Evaluate the model and visualize the results.\"\"\"\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        color_mode=\"grayscale\",\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # Get predictions and calculate metrics\n",
        "    predictions = model.predict(test_generator)\n",
        "    true_labels = test_generator.classes\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    report = classification_report(\n",
        "        true_labels, predicted_labels, target_names=expressions\n",
        "    )\n",
        "    accuracy = np.sum(true_labels == predicted_labels) / len(true_labels)\n",
        "\n",
        "    print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=expressions,\n",
        "        yticklabels=expressions,\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Accuracy and Loss over Epochs\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "OJLtUJJDLeND"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    types = ['vgg16','vgg19']\n",
        "    for tp in types:\n",
        "      print(f\"Model type: {tp}\")\n",
        "      # Build the model\n",
        "      model = build_model(tp)\n",
        "\n",
        "      # Train the model (optional, you can load a pretrained model)\n",
        "      train = input(\"Do you want to train the model? (y/n): \")\n",
        "      if train.lower() == 'y':\n",
        "          history = train_model(tp,model, train_data_dir, test_data_dir, epochs=60)\n",
        "      else:\n",
        "          # Load pre-trained model (if available)\n",
        "          if os.path.exists(f\"model_{tp}.h5\"):\n",
        "              model = load_model(\"model_CNN.h5\")\n",
        "              print(\"Model loaded successfully.\")\n",
        "          else:\n",
        "              print(\"No pre-trained model found. You should train the model first.\")\n",
        "              return\n",
        "\n",
        "      # Evaluate the model on the test set\n",
        "      evaluate = input(\"Do you want to evaluate the model? (y/n): \")\n",
        "      if evaluate.lower() == 'y':\n",
        "          evaluate_model(model, test_data_dir, history)\n",
        "\n",
        "      # Run real-time facial expression detection\n",
        "      run_realtime_choice = input(\"Do you want to run real-time facial expression detection? (y/n): \")\n",
        "      if run_realtime_choice.lower() == 'y':\n",
        "          run_realtime(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXR7kiWFLeNE",
        "outputId": "6bad6b49-e5a3-41ca-ff57-683ec7035e67"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model type: vgg16\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m58889256/58889256\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 0us/step\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "28709\n",
            "7178\n",
            "Epoch 1/60\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 53ms/step - accuracy: 0.2830 - loss: 1.7595 - val_accuracy: 0.3418 - val_loss: 1.6607\n",
            "Epoch 2/60\n",
            "\u001b[1m  1/897\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 14ms/step - accuracy: 0.3438 - loss: 1.7145"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/epoch_iterator.py:107: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self._interrupted_warning()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3438 - loss: 1.7145 - val_accuracy: 0.3417 - val_loss: 1.6638\n",
            "Epoch 3/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 60ms/step - accuracy: 0.3214 - loss: 1.6871 - val_accuracy: 0.3612 - val_loss: 1.6234\n",
            "Epoch 4/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4062 - loss: 1.5622 - val_accuracy: 0.3643 - val_loss: 1.6220\n",
            "Epoch 5/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 50ms/step - accuracy: 0.3370 - loss: 1.6671 - val_accuracy: 0.3637 - val_loss: 1.6150\n",
            "Epoch 6/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.4062 - loss: 1.7025 - val_accuracy: 0.3608 - val_loss: 1.6200\n",
            "Epoch 7/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 56ms/step - accuracy: 0.3384 - loss: 1.6629 - val_accuracy: 0.3774 - val_loss: 1.5906\n",
            "Epoch 8/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4688 - loss: 1.6942 - val_accuracy: 0.3775 - val_loss: 1.5901\n",
            "Epoch 9/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - accuracy: 0.3432 - loss: 1.6519 - val_accuracy: 0.3665 - val_loss: 1.6103\n",
            "Epoch 10/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3125 - loss: 1.8061 - val_accuracy: 0.3673 - val_loss: 1.6073\n",
            "Epoch 11/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 48ms/step - accuracy: 0.3442 - loss: 1.6443 - val_accuracy: 0.3790 - val_loss: 1.5765\n",
            "Epoch 12/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3125 - loss: 1.8200 - val_accuracy: 0.3795 - val_loss: 1.5770\n",
            "Epoch 13/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 51ms/step - accuracy: 0.3479 - loss: 1.6473 - val_accuracy: 0.3811 - val_loss: 1.5818\n",
            "Epoch 14/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.4375 - loss: 1.5193 - val_accuracy: 0.3790 - val_loss: 1.5829\n",
            "Epoch 15/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 55ms/step - accuracy: 0.3518 - loss: 1.6336 - val_accuracy: 0.3772 - val_loss: 1.5873\n",
            "Epoch 16/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.3125 - loss: 1.6609 - val_accuracy: 0.3774 - val_loss: 1.5867\n",
            "Epoch 17/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 49ms/step - accuracy: 0.3536 - loss: 1.6282 - val_accuracy: 0.3834 - val_loss: 1.5807\n",
            "Epoch 18/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.1875 - loss: 1.6069 - val_accuracy: 0.3823 - val_loss: 1.5824\n",
            "Epoch 19/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.3576 - loss: 1.6326 - val_accuracy: 0.3823 - val_loss: 1.5829\n",
            "Epoch 20/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.2812 - loss: 1.6764 - val_accuracy: 0.3817 - val_loss: 1.5805\n",
            "Epoch 21/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.3555 - loss: 1.6247 - val_accuracy: 0.3825 - val_loss: 1.5725\n",
            "Epoch 22/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3750 - loss: 1.7486 - val_accuracy: 0.3859 - val_loss: 1.5715\n",
            "Epoch 23/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 92ms/step - accuracy: 0.3606 - loss: 1.6131 - val_accuracy: 0.3892 - val_loss: 1.5639\n",
            "Epoch 24/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.3750 - loss: 1.5601 - val_accuracy: 0.3888 - val_loss: 1.5641\n",
            "Epoch 25/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 47ms/step - accuracy: 0.3577 - loss: 1.6214 - val_accuracy: 0.3892 - val_loss: 1.5700\n",
            "Epoch 26/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.2188 - loss: 1.7733 - val_accuracy: 0.3876 - val_loss: 1.5686\n",
            "Epoch 27/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 50ms/step - accuracy: 0.3545 - loss: 1.6219 - val_accuracy: 0.3905 - val_loss: 1.5641\n",
            "Epoch 28/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4062 - loss: 1.6576 - val_accuracy: 0.3913 - val_loss: 1.5633\n",
            "Epoch 29/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 54ms/step - accuracy: 0.3584 - loss: 1.6162 - val_accuracy: 0.3789 - val_loss: 1.6050\n",
            "Epoch 30/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4062 - loss: 1.7672 - val_accuracy: 0.3772 - val_loss: 1.6068\n",
            "Epoch 31/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 58ms/step - accuracy: 0.3637 - loss: 1.6100 - val_accuracy: 0.3920 - val_loss: 1.5712\n",
            "Epoch 32/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3438 - loss: 1.5319 - val_accuracy: 0.3941 - val_loss: 1.5687\n",
            "Epoch 33/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 62ms/step - accuracy: 0.3598 - loss: 1.6129 - val_accuracy: 0.3864 - val_loss: 1.5749\n",
            "Epoch 34/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.3438 - loss: 1.6186 - val_accuracy: 0.3873 - val_loss: 1.5753\n",
            "Epoch 35/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 65ms/step - accuracy: 0.3627 - loss: 1.6127 - val_accuracy: 0.3892 - val_loss: 1.5683\n",
            "Epoch 36/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.4062 - loss: 1.5961 - val_accuracy: 0.3913 - val_loss: 1.5674\n",
            "Epoch 37/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 69ms/step - accuracy: 0.3668 - loss: 1.6019 - val_accuracy: 0.3954 - val_loss: 1.5576\n",
            "Epoch 38/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.2812 - loss: 1.8341 - val_accuracy: 0.3938 - val_loss: 1.5575\n",
            "Epoch 39/60\n",
            "\u001b[1m897/897\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step - accuracy: 0.3606 - loss: 1.6119"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}