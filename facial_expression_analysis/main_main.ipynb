{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "Ek_azYDeLeM1"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import mediapipe as mp\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import os\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "pw-zkxryLeM4"
      },
      "outputs": [],
      "source": [
        "# Expressions labels\n",
        "expressions = [\"anger\", \"disgust\", \"fear\", \"happy\", \"sad\", \"surprise\", \"neutral\"]\n",
        "train_data_dir = '/content/Dataset/train/'\n",
        "test_data_dir = '/content/Dataset/test/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "2WsY27kbLeM5"
      },
      "outputs": [],
      "source": [
        "# Moving window for smoothing predictions\n",
        "PREDICTION_WINDOW = deque(maxlen=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "bVT4TpCBLeM6"
      },
      "outputs": [],
      "source": [
        "# Initialize MediaPipe Face Mesh\n",
        "mp_face_mesh = mp.solutions.face_mesh\n",
        "face_mesh = mp_face_mesh.FaceMesh(\n",
        "    static_image_mode=False, max_num_faces=5, min_detection_confidence=0.5\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "Gxdy5omILeM7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import VGG16, VGG19\n",
        "\n",
        "\n",
        "def build_model(type):\n",
        "    \"\"\"Build the CNN model.\"\"\"\n",
        "    #  Load VGG19 with pretrained ImageNet weights\n",
        "    if type == 'vgg16':\n",
        "        base_model = VGG16(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "    else:\n",
        "        base_model = VGG19(weights='imagenet', include_top=False, input_shape=(48, 48, 3))\n",
        "\n",
        "\n",
        "    # Freeze the base layers\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom classification head\n",
        "    x = GlobalAveragePooling2D()(base_model.output)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    output = Dense(7, activation='softmax')(x)  # Example: 10 classes\n",
        "\n",
        "    # Create new model\n",
        "    model = Model(inputs=base_model.input, outputs=output)\n",
        "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "# build_model()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(tp,model, train_dir, test_dir, epochs=50):\n",
        "    \"\"\"Train the model with given data.\"\"\"\n",
        "    # Data augmentation\n",
        "    train_datagen = ImageDataGenerator(\n",
        "        rescale=1. / 255,\n",
        "        rotation_range=30,\n",
        "        shear_range=0.3,\n",
        "        zoom_range=0.3,\n",
        "        width_shift_range=0.2,\n",
        "        height_shift_range=0.2,\n",
        "        horizontal_flip=True,\n",
        "        fill_mode=\"nearest\",\n",
        "    )\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(\n",
        "        train_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "    validation_generator = test_datagen.flow_from_directory(\n",
        "        test_data_dir,\n",
        "        color_mode=\"rgb\",\n",
        "        target_size=(48,48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        shuffle = True\n",
        "    )\n",
        "\n",
        "    num_train_imgs = 0\n",
        "    for root, dirs, files in os.walk(train_data_dir):\n",
        "        num_train_imgs += len(files)\n",
        "    num_test_imgs = 0\n",
        "    for root, dirs, files in os.walk(test_data_dir):\n",
        "        num_test_imgs += len(files)\n",
        "\n",
        "    print(num_train_imgs)\n",
        "    print(num_test_imgs)\n",
        "\n",
        "    history = model.fit(\n",
        "        train_generator,\n",
        "        steps_per_epoch=num_train_imgs // 32,\n",
        "        epochs=epochs,\n",
        "        validation_data=validation_generator,\n",
        "        validation_steps=num_test_imgs // 32,\n",
        "\n",
        "    )\n",
        "\n",
        "    model.save(f\"model_{tp}.h5\")\n",
        "\n",
        "    return history"
      ],
      "metadata": {
        "id": "qhYoXg8ZOE-s"
      },
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "gD0iish2LeM_"
      },
      "outputs": [],
      "source": [
        "def preprocess_face(face_img):\n",
        "    \"\"\"Preprocess a face image for prediction.\"\"\"\n",
        "    gray = cv2.cvtColor(face_img, cv2.COLOR_BGR2GRAY)\n",
        "    resized = cv2.resize(gray, (48, 48))\n",
        "    normalized = resized / 255.0\n",
        "    return normalized.reshape(1, 48, 48, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "0gruQbXBLeNA"
      },
      "outputs": [],
      "source": [
        "def detect_expression(frame, model):\n",
        "    \"\"\"Detect facial expression in a video frame.\"\"\"\n",
        "    height, width, _ = frame.shape\n",
        "    rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "    results = face_mesh.process(rgb_frame)\n",
        "\n",
        "    if results.multi_face_landmarks:\n",
        "        for face_landmarks in results.multi_face_landmarks:\n",
        "            x_min = y_min = float(\"inf\")\n",
        "            x_max = y_max = float(\"-inf\")\n",
        "\n",
        "            for landmark in face_landmarks.landmark:\n",
        "                x, y = int(landmark.x * width), int(landmark.y * height)\n",
        "                x_min = min(x_min, x)\n",
        "                x_max = max(x_max, x)\n",
        "                y_min = min(y_min, y)\n",
        "                y_max = max(y_max, y)\n",
        "\n",
        "            padding = 20\n",
        "            x_min = max(0, x_min - padding)\n",
        "            y_min = max(0, y_min - padding)\n",
        "            x_max = min(width, x_max + padding)\n",
        "            y_max = min(height, y_max + padding)\n",
        "\n",
        "            face = frame[y_min:y_max, x_min:x_max]\n",
        "            if face.size == 0:\n",
        "                continue\n",
        "\n",
        "            processed_face = preprocess_face(face)\n",
        "            prediction = model.predict(processed_face)[0]\n",
        "            PREDICTION_WINDOW.append(prediction)\n",
        "\n",
        "            avg_prediction = np.mean(PREDICTION_WINDOW, axis=0)\n",
        "            expression = expressions[np.argmax(avg_prediction)]\n",
        "            confidence = float(avg_prediction.max())\n",
        "\n",
        "            cv2.rectangle(frame, (x_min, y_min), (x_max, y_max), (0, 255, 0), 2)\n",
        "            cv2.putText(\n",
        "                frame,\n",
        "                f\"{expression} ({confidence:.2f})\",\n",
        "                (x_min, y_min - 10),\n",
        "                cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                0.9,\n",
        "                (0, 255, 0),\n",
        "                2,\n",
        "            )\n",
        "\n",
        "    return frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "MkudQH-5LeNB"
      },
      "outputs": [],
      "source": [
        "def run_realtime(model):\n",
        "    \"\"\"Run real-time facial expression detection.\"\"\"\n",
        "    cap = cv2.VideoCapture(0)\n",
        "    while True:\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        frame = cv2.flip(frame, 1)\n",
        "        processed_frame = detect_expression(frame, model)\n",
        "\n",
        "        cv2.imshow(\"Facial Expression Detection\", processed_frame)\n",
        "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
        "            break\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "3PYYVmzFLeNC"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(model, test_dir, history):\n",
        "    \"\"\"Evaluate the model and visualize the results.\"\"\"\n",
        "    test_datagen = ImageDataGenerator(rescale=1.0 / 255)\n",
        "    test_generator = test_datagen.flow_from_directory(\n",
        "        test_dir,\n",
        "        target_size=(48, 48),\n",
        "        batch_size=32,\n",
        "        class_mode=\"categorical\",\n",
        "        color_mode=\"grayscale\",\n",
        "        shuffle=False,\n",
        "    )\n",
        "\n",
        "    # Get predictions and calculate metrics\n",
        "    predictions = model.predict(test_generator)\n",
        "    true_labels = test_generator.classes\n",
        "    predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "    cm = confusion_matrix(true_labels, predicted_labels)\n",
        "    report = classification_report(\n",
        "        true_labels, predicted_labels, target_names=expressions\n",
        "    )\n",
        "    accuracy = np.sum(true_labels == predicted_labels) / len(true_labels)\n",
        "\n",
        "    print(f\"Overall Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(report)\n",
        "\n",
        "    # Plot Confusion Matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(\n",
        "        cm,\n",
        "        annot=True,\n",
        "        fmt=\"d\",\n",
        "        cmap=\"Blues\",\n",
        "        xticklabels=expressions,\n",
        "        yticklabels=expressions,\n",
        "    )\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"True\")\n",
        "    plt.show()\n",
        "\n",
        "    # Plot Accuracy and Loss over Epochs\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    # Plot Accuracy\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
        "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
        "    plt.title('Accuracy Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    # Plot Loss\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Loss Over Epochs')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "OJLtUJJDLeND"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "\n",
        "    types = ['vgg16','vgg19']\n",
        "    for tp in types:\n",
        "      print(f\"Model type: {tp}\")\n",
        "      # Build the model\n",
        "      model = build_model(tp)\n",
        "\n",
        "      # Train the model (optional, you can load a pretrained model)\n",
        "      train = input(\"Do you want to train the model? (y/n): \")\n",
        "      if train.lower() == 'y':\n",
        "          history = train_model(tp,model, train_data_dir, test_data_dir, epochs=60)\n",
        "      else:\n",
        "          # Load pre-trained model (if available)\n",
        "          if os.path.exists(f\"model_{tp}.h5\"):\n",
        "              model = load_model(\"model_CNN.h5\")\n",
        "              print(\"Model loaded successfully.\")\n",
        "          else:\n",
        "              print(\"No pre-trained model found. You should train the model first.\")\n",
        "              return\n",
        "\n",
        "      # Evaluate the model on the test set\n",
        "      evaluate = input(\"Do you want to evaluate the model? (y/n): \")\n",
        "      if evaluate.lower() == 'y':\n",
        "          evaluate_model(model, test_data_dir, history)\n",
        "\n",
        "      # Run real-time facial expression detection\n",
        "      run_realtime_choice = input(\"Do you want to run real-time facial expression detection? (y/n): \")\n",
        "      if run_realtime_choice.lower() == 'y':\n",
        "          run_realtime(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXR7kiWFLeNE",
        "outputId": "c2eb02b9-c04c-4c6e-c0cd-6550c3975995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model type: vgg16\n",
            "Do you want to train the model? (y/n): y\n",
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "28709\n",
            "7178\n",
            "Epoch 1/60\n",
            "\u001b[1m  8/897\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13:59\u001b[0m 944ms/step - accuracy: 0.1410 - loss: 2.0233"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}